{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the spam training data set: (1000, 55)\n",
      "[[1 0 0 ... 0 0 0]\n",
      " [0 0 1 ... 1 0 0]\n",
      " [0 0 0 ... 1 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 1]\n",
      " [1 1 1 ... 1 1 0]\n",
      " [1 0 0 ... 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "training_spam = np.loadtxt(open(\"data/training_spam.csv\"), delimiter=\",\").astype(int)\n",
    "print(\"Shape of the spam training data set:\", training_spam.shape)\n",
    "print(training_spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the spam testing data set: (500, 55)\n",
      "[[1 0 0 ... 1 1 1]\n",
      " [1 1 0 ... 1 1 1]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 1 0 0]\n",
      " [0 0 0 ... 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "testing_spam = np.loadtxt(open(\"data/testing_spam.csv\"), delimiter=\",\").astype(int)\n",
    "print(\"Shape of the spam testing data set:\", testing_spam.shape)\n",
    "print(testing_spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.48939034, -0.94933059]),\n",
       " array([[-3.3738029 , -3.62650526, -3.10696769, -4.08655855, -3.22521552,\n",
       "         -3.60792887, -3.9831802 , -3.76003665, -3.67124115, -3.39837416,\n",
       "         -3.78169815, -2.80727624, -3.50872222, -3.85749198, -3.99214887,\n",
       "         -3.62650526, -3.60181264, -3.50872222, -2.6081093 , -3.9831802 ,\n",
       "         -2.96833779, -4.04771872, -3.94808888, -3.9831802 , -2.8995607 ,\n",
       "         -3.09956025, -3.12952504, -3.42867951, -3.50872222, -3.454655  ,\n",
       "         -3.62027471, -3.71124648, -3.58368526, -3.7044668 , -3.43382091,\n",
       "         -3.39341137, -3.08490797, -3.99214887, -3.57178036, -3.62650526,\n",
       "         -3.85749198, -3.5368931 , -3.60181264, -3.62027471, -3.06688947,\n",
       "         -3.38355908, -3.99214887, -3.78902419, -3.3450928 , -2.61717966,\n",
       "         -3.45993206, -3.07766156, -3.60792887, -3.67779855],\n",
       "        [-3.15803777, -3.26974176, -2.86125093, -3.99993021, -2.79690124,\n",
       "         -3.10268767, -3.11429322, -3.18690575, -3.29273127, -3.0179224 ,\n",
       "         -3.29273127, -2.82855856, -3.25172325, -3.66615703, -3.6264167 ,\n",
       "         -2.89504831, -3.06865792, -3.20378379, -2.56081071, -3.43226068,\n",
       "         -2.63750333, -3.89268468, -3.2428343 , -3.12210576, -3.9538391 ,\n",
       "         -3.98123807, -4.04824878, -4.00940895, -4.0189784 , -4.02864031,\n",
       "         -4.04824878, -4.05819912, -3.94487043, -4.05819912, -3.91843717,\n",
       "         -3.81918222, -3.85118495, -3.99993021, -3.90977911, -3.63948878,\n",
       "         -4.05819912, -4.03839649, -3.84308774, -4.0189784 , -3.33549313,\n",
       "         -3.94487043, -4.00940895, -4.02864031, -3.5696191 , -2.81693053,\n",
       "         -3.75809452, -2.59958409, -2.82272764, -3.33549313]]))"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This skeleton code simply classifies every input as ham\n",
    "#\n",
    "# Here you can see there is a parameter k that is unused, the\n",
    "# point is to show you how you could set up your own. You can\n",
    "# also see a train method that does nothing here\n",
    "# but your classifier would probably do the main work here. \n",
    "# Modify this code as much as you like so long as the \n",
    "# accuracy test in the cell below runs\n",
    "\n",
    "class MyClassifier:\n",
    "    def __init__(self, k):\n",
    "        # K shall be used to define the number of classes in the classification problem\n",
    "        self.k = k\n",
    "        # Declaring an array called log_class_priors which will hold the log of the class priors\n",
    "        self.log_class_priors = np.array([])\n",
    "        # Declaring an array called theta which will hold the log of the class conditional feature likelihoods\n",
    "        self.theta = np.array([])\n",
    "        \n",
    "    def estimate_log_class_priors(self, data):\n",
    "        \"\"\"\n",
    "        Given a data set with binary response variable (0s and 1s), \n",
    "        calculate the logarithm of the empirical class priors,\n",
    "        that is, the logarithm of the proportions of 0s and 1s:\n",
    "            log(p(C=0)) and log(p(C=1))\n",
    "\n",
    "        :param data: a numpy array of length n_samples\n",
    "                     that contains the binary response (coded as 0s and 1s).\n",
    "\n",
    "        :return log_class_priors: a numpy array of length two\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE...\n",
    "\n",
    "        # Defining np array\n",
    "        out = np.array([])\n",
    "        \n",
    "        # Finding the probability of each class in range (0, k-1)\n",
    "        for i in range(self.k):\n",
    "            out = np.append(out, (np.count_nonzero(data == i)/len(data)))\n",
    "        \n",
    "        # Getting the log of each element in out  \n",
    "        out = np.log(out)\n",
    "\n",
    "        # Returning the np array\n",
    "        return out\n",
    "    \n",
    "    def estimate_log_class_conditional_likelihoods(self, input_data, labels, alpha=1.0):\n",
    "        \"\"\"\n",
    "        Given input_data of binary features (words) and labels \n",
    "        (binary response variable (0s and 1s)), calculate the logarithm \n",
    "        of the empirical class-conditional likelihoods, that is,\n",
    "        log(P(w_i | c)) for all features w_i and both classes (c in {0, 1}).\n",
    "\n",
    "        Assume a multinomial feature distribution and use Laplace smoothing\n",
    "        if alpha > 0.\n",
    "\n",
    "        :param input_data: a two-dimensional numpy-array with shape = [n_samples, n_features]\n",
    "                           contains binary features (words)\n",
    "        :param labels: a numpy array of length n_samples \n",
    "                       contains response variable\n",
    "\n",
    "        :return theta:\n",
    "            a numpy array of shape = [2, n_features]. theta[j, i] corresponds to the\n",
    "            logarithm of the probability of feature i appearing in a sample belonging \n",
    "            to class j.\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE...\n",
    "\n",
    "        # Getting the magnitude of the dimensions\n",
    "        n_samples = input_data.shape[0]\n",
    "        n_features = input_data.shape[1]\n",
    "        \n",
    "        # Declaring the arrays for the calculations\n",
    "        classes = []\n",
    "        class_conditionals = []\n",
    "        log_class_conditionals = []\n",
    "        theta = []\n",
    "        \n",
    "        # Looping through each of the classes\n",
    "        for i in range(self.k):\n",
    "            # Isolating all the lines in input_data where class label == i, where i is in the range (0, k-1)\n",
    "            # np.squeeze() removes all the redundant dimensions\n",
    "            classes.append(np.squeeze(input_data[np.where(labels == i), :]))\n",
    "            # Calculating the empircal conditional liklihoods for class = i (c == i)\n",
    "            class_conditionals.append(((np.count_nonzero(classes[i], axis=0)) + alpha) / ((np.count_nonzero(labels == i)) + n_features*alpha))\n",
    "            # Finding the log of class_conditionals[i]\n",
    "            log_class_conditionals.append(np.log(class_conditionals[i]))\n",
    "            # Appending each of the class conditional likelihoods to an array called 'theta'\n",
    "            theta.append(log_class_conditionals[i])\n",
    "            \n",
    "        # Converting theta to a numpy array and returning it\n",
    "        return np.array(theta)\n",
    "        \n",
    "    def train(self, train_data, train_labels):\n",
    "        \n",
    "        # Calculating the log of the probability of each class\n",
    "        self.log_class_priors = self.estimate_log_class_priors(train_labels)\n",
    "        # Calculating the log of the class conditional feature likelihoods\n",
    "        # Remember to change alpha to optimise the training process\n",
    "        self.theta = self.estimate_log_class_conditional_likelihoods(train_data, train_labels, alpha=100)\n",
    "        \n",
    "        # Remember to remove this line when you copy it back into the main file\n",
    "        return self.log_class_priors, self.theta\n",
    "        \n",
    "    def predict(self, test_data):\n",
    "        \"\"\"\n",
    "        Given a new data set with binary features, predict the corresponding\n",
    "        response for each instance (row) of the new_data set.\n",
    "\n",
    "        :param new_data: a two-dimensional numpy-array with shape = [n_test_samples, n_features].\n",
    "        :param log_class_priors: a numpy array of length 2.\n",
    "        :param log_class_conditional_likelihoods: a numpy array of shape = [2, n_features].\n",
    "            theta[j, i] corresponds to the logarithm of the probability of feature i appearing\n",
    "            in a sample belonging to class j.\n",
    "        :return class_predictions: a numpy array containing the class predictions for each row\n",
    "            of new_data.\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE...\n",
    "\n",
    "        class_predictions = np.array([])\n",
    "\n",
    "        # Looping through each row in new_data\n",
    "        for i, row in enumerate(test_data):\n",
    "            # Summing the log_class_conditional_likelihoods for all the features in the dataset\n",
    "            row_times_class_conditionals = row*self.theta\n",
    "            row_class_conditional_likelihoods_sum = np.sum(row_times_class_conditionals, axis=1)\n",
    "\n",
    "            # Adding the row_class_conditional_likelihoods_sum to the log_class_priors\n",
    "            log_total = row_class_conditional_likelihoods_sum + self.log_class_priors\n",
    "            # Finding the class which has the highest probability and appending it to class predictions\n",
    "            class_predictions = np.append(class_predictions, np.argmax(log_total, axis=0))\n",
    "\n",
    "        return class_predictions\n",
    "\n",
    "# Change k to define the number of classes in the classification problem\n",
    "spam_classifier = MyClassifier(k=2)\n",
    "spam_classifier.train(train_data=training_spam[:, 1:], train_labels=training_spam[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result [-0.48939034 -0.94933059]\n"
     ]
    }
   ],
   "source": [
    "my_classifier = MyClassifier(k=2)\n",
    "\n",
    "# You can use this cell to check whether the returned objects of your function are of the right data type.\n",
    "log_class_priors = my_classifier.estimate_log_class_priors(training_spam[:, 0])\n",
    "print(\"result\", log_class_priors)\n",
    "\n",
    "# Check length\n",
    "assert(len(log_class_priors) == 2)\n",
    "\n",
    "# Check whether the returned object is a numpy.ndarray\n",
    "assert(isinstance(log_class_priors, np.ndarray))\n",
    "\n",
    "# Check wehther the values of this numpy.array are floats.\n",
    "assert(log_class_priors.dtype == float)\n",
    "\n",
    "# Check wehther the values are both negative (the logarithm of a probability 0 < p < 1 should be negative).\n",
    "assert(np.all(log_class_priors < 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.82996121 -2.39191618 -1.36699161 -5.80964287 -1.56114762 -2.34390696\n",
      "  -3.93784069 -2.78921798 -2.513806   -1.87781723 -2.86520389 -0.92684094\n",
      "  -2.10834089 -3.17058554 -4.0178834  -2.39191618 -2.32840278 -2.10834089\n",
      "  -0.66214839 -3.93784069 -1.15568252 -4.71103058 -3.6695767  -3.93784069\n",
      "  -1.05605267 -1.35529557 -1.40292362 -1.93844185 -2.10834089 -1.99193054\n",
      "  -2.37565566 -2.63158904 -2.28328234 -2.61096975 -1.94891315 -1.86806106\n",
      "  -1.33230605 -4.0178834  -2.2542948  -2.39191618 -3.17058554 -2.17205671\n",
      "  -2.32840278 -2.37565566 -1.30429301 -1.8488297  -4.0178834  -2.89187213\n",
      "  -1.77540223 -0.67384443 -2.00298038 -1.3210065  -2.34390696 -2.53249813]\n",
      " [-1.09861229 -1.29325433 -0.6423075  -4.14313473 -0.55171061 -1.00764051\n",
      "  -1.02644984 -1.14740245 -1.33545468 -0.87410912 -1.33545468 -0.59598343\n",
      "  -1.26073114 -2.19722458 -2.08171169 -0.69088217 -0.95324644 -1.17638999\n",
      "  -0.2368424  -1.61170806 -0.33647224 -3.1446059  -1.24485779 -1.03918887\n",
      "  -3.60413823 -3.8918203  -5.39589769 -4.29728541 -4.47960696 -4.70275051\n",
      "  -5.39589769 -6.08904488 -3.52409552 -6.08904488 -3.31645615 -2.75684037\n",
      "  -2.91099105 -4.14313473 -3.25583153 -2.11875296 -6.08904488 -4.99043259\n",
      "  -2.87016905 -4.47960696 -1.41621604 -3.52409552 -4.29728541 -4.70275051\n",
      "  -1.93016179 -0.57965654 -2.50552594 -0.2869265  -0.58778666 -1.41621604]]\n"
     ]
    }
   ],
   "source": [
    "# You can use this cell to check whether the returned objects of your function are of the right data type.\n",
    "log_class_conditional_likelihoods = my_classifier.estimate_log_class_conditional_likelihoods(\n",
    "    training_spam[:, 1:], training_spam[:, 0], alpha=1.0)\n",
    "print(log_class_conditional_likelihoods)\n",
    "\n",
    "# Check data type(s)\n",
    "assert(isinstance(log_class_conditional_likelihoods, np.ndarray))\n",
    "\n",
    "# Check shape of numpy array\n",
    "assert(log_class_conditional_likelihoods.shape == (2, 54))\n",
    "\n",
    "# Check data type of array elements\n",
    "assert(log_class_conditional_likelihoods.dtype == float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log class priors\n",
      "[-0.48939034 -0.94933059]\n",
      "log class conditional likelihoods\n",
      "[[-3.3738029  -3.62650526 -3.10696769 -4.08655855 -3.22521552 -3.60792887\n",
      "  -3.9831802  -3.76003665 -3.67124115 -3.39837416 -3.78169815 -2.80727624\n",
      "  -3.50872222 -3.85749198 -3.99214887 -3.62650526 -3.60181264 -3.50872222\n",
      "  -2.6081093  -3.9831802  -2.96833779 -4.04771872 -3.94808888 -3.9831802\n",
      "  -2.8995607  -3.09956025 -3.12952504 -3.42867951 -3.50872222 -3.454655\n",
      "  -3.62027471 -3.71124648 -3.58368526 -3.7044668  -3.43382091 -3.39341137\n",
      "  -3.08490797 -3.99214887 -3.57178036 -3.62650526 -3.85749198 -3.5368931\n",
      "  -3.60181264 -3.62027471 -3.06688947 -3.38355908 -3.99214887 -3.78902419\n",
      "  -3.3450928  -2.61717966 -3.45993206 -3.07766156 -3.60792887 -3.67779855]\n",
      " [-3.15803777 -3.26974176 -2.86125093 -3.99993021 -2.79690124 -3.10268767\n",
      "  -3.11429322 -3.18690575 -3.29273127 -3.0179224  -3.29273127 -2.82855856\n",
      "  -3.25172325 -3.66615703 -3.6264167  -2.89504831 -3.06865792 -3.20378379\n",
      "  -2.56081071 -3.43226068 -2.63750333 -3.89268468 -3.2428343  -3.12210576\n",
      "  -3.9538391  -3.98123807 -4.04824878 -4.00940895 -4.0189784  -4.02864031\n",
      "  -4.04824878 -4.05819912 -3.94487043 -4.05819912 -3.91843717 -3.81918222\n",
      "  -3.85118495 -3.99993021 -3.90977911 -3.63948878 -4.05819912 -4.03839649\n",
      "  -3.84308774 -4.0189784  -3.33549313 -3.94487043 -4.00940895 -4.02864031\n",
      "  -3.5696191  -2.81693053 -3.75809452 -2.59958409 -2.82272764 -3.33549313]]\n"
     ]
    }
   ],
   "source": [
    "# You can use this cell to check whether the returned objects of your function are of the right data type.\n",
    "log_class_priors, log_class_conditional_likelihoods = my_classifier.train(training_spam[:, 1:], training_spam[:, 0])\n",
    "print(\"log class priors\")\n",
    "print(log_class_priors)\n",
    "\n",
    "# Check length\n",
    "assert(len(log_class_priors) == 2)\n",
    "\n",
    "# Check whether the returned object is a numpy.ndarray\n",
    "assert(isinstance(log_class_priors, np.ndarray))\n",
    "\n",
    "# Check wehther the values of this numpy.array are floats.\n",
    "assert(log_class_priors.dtype == float)\n",
    "\n",
    "# Check wehther the values are both negative (the logarithm of a probability 0 < p < 1 should be negative).\n",
    "assert(np.all(log_class_priors < 0))\n",
    "\n",
    "print(\"log class conditional likelihoods\")\n",
    "print(log_class_conditional_likelihoods)\n",
    "\n",
    "# Check data type(s)\n",
    "assert(isinstance(log_class_conditional_likelihoods, np.ndarray))\n",
    "\n",
    "# Check shape of numpy array\n",
    "assert(log_class_conditional_likelihoods.shape == (2, 54))\n",
    "\n",
    "# Check data type of array elements\n",
    "assert(log_class_conditional_likelihoods.dtype == float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
